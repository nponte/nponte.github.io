<!DOCTYPE HTML>
<html>

<head>
    <title>H H Hash (Final Report)</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
    <link rel="stylesheet" href="assets/css/main.css" />
    <!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
    <!--[if lte IE 9]><link rel="stylesheet" href="assets/css/ie9.css" /><![endif]-->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML">
</script>
</head>

<body>

    <!-- Header -->
    <section id="header">
        <header class="major">
            <h1>H H HASH</h1>
            <p style="font-weight: 700"><span>Final Report</span></p>
            <p style="margin-top: 1.8em; font-weight: 500">Norman Ponte (nponte) &nbsp;&nbsp; Yiming Zong (yzong)</p>

            <p style="margin-top: 3.5em;">
                <span style="font-weight: 300; text-transform:none;"><a href="/checkpoint.html">Click here</a> for our proposal &amp; checkpoint writeup.<br />
                <span style="font-weight: 300; text-transform:none;">Check out our <a href="/assets/hhhash-final.zip">source code and benchmark data</a> and <a href="/assets/final-presentation.pdf">final presentation</a>.
                </span>
            </p>
        </header>
    </section>

    <!-- One -->

    <section id="one" class="main special">
        <div class="container">
            <span class="image fit primary"><img src="images/test1.jpg" alt="" /></span>
            <div class="content">
                <header class="major">
                    <h2>Summary</h2>
                </header>
                <p>The goal of our project is to implement a GPU-based hash table that uses two-level cuckoo hashing, and we compare its performance against fine-tuned, CPU-based hash table implementation. We built HH Hash from scratch in order to have full control over GPU memory management, hash function, and bucket-rebalancing. And, we benchmarked the performance of the hash table with batches of insertions, lookups, and deletions, and measured the scalability of our design. Our source code can be compiled and run on <code>latedays</code> cluster (especially the Tesla and Titanx workers).</p>
                <p>In this writeup, we first describe how two-level cuckoo hashing works and demonstrate how we parallelized the algorithm on TitanX GPU. Then, we present the benchmark result of our hash table, compare it with CPU-based hash table (NBDS), and eventaully conclude that HH Hash gives great performance improvements for huge, batchable workloads, while traditional CPU-based hash tables work better for smaller datasets and individual operations.</p>
            </div>
            <a href="#two" class="goto-next scrolly">Next</a>
        </div>
    </section>

    <!-- Second One -->
    <section id="two" class="main special">
        <div class="container">
            <span class="image fit primary"><img src="images/test1.jpg" alt="" /></span>
            <div class="content">
                <header class="major">
                    <h2>Background</h2>
                </header>
                <p>The data structure that we try to parallelize in this project is a hash table. A hash table manages a collection of key-value pairs by supporting the three operations: <code>insert(k,v)</code>, <code>lookup(k)</code>, and <code>delete(k)</code>. The keys and values can be of arbitrary types as long as the key is hashable, and the advantage of a hash table over, say, binary search on a sorted list is that hash table allows all three operations to be completed in constant-time (on average). In fact, hash table is but an abstraction, and it can be implemented using different techniques, including linear/quadratic probing, separate chaining, and cuckoo hashing. And, for this project we focus on parallelizing cuckoo hashing on a GPU because the problem is neither trivial (topic of recent PhD dissertation) nor widely-solved (no public source code available).</p>

                <p> $m$-Cuckoo hashing is an open-addressing hashing strategy that maps a key to one of $m$ hash value candidates, i.e. $\{ h_i(k) \mid i\in[m] \}$. Here is what happens when one attempts to insert <code>(k,v)</code> into the hash table: the hash table checks if the slot $h_0(k)$ is already occupied -- if not, we place <code>(k,v)</code> there and succeed; otherwise, we replace the entry there with <code>(k,v)</code>, and then attempt to hash the original key-value pair with an alternative hash function, say, $h_1(k')$. Notice that this is a highly sequential process, since the behavior at each step is determined by whether the key-value pair in the previous step was placed in an empty spot, and, if not, which key-value pair was evicted. The <a href="https://en.wikipedia.org/wiki/Cuckoo_hashing">Wikipedia page</a> for Cuckoo Hashing contains several examples for motivated readers.</p>

                <p>The naive sequential algorithm can be inefficient not only due to lack of parallelism but also due to poor cache locality. When some <code>(k,v)</code> is being inserted, it may swap with another key-value pair <i>anywhere</i> on the hash table, since the value of $h_i(k)$ is not bounded. This results in random read/write pattern on the hash table, which may cause significant cache line thrashing. However, we claim that cuckoo hashing can be parallelized in order to help hide the memory latency. Instead of inserting elements one after another, we can perform <i>batch insertions</i>, as follows: firstly, we hash every element to be inserted by its first hashing function, place them in the hash table, while potentially swapping out some existing entries. Then, anything that was swapped out are hashed with the second function, and the same swapping process repeats.</p>

                 <p>On top of $m$-Cuckoo hashing, we also use FKS hashing to augment our hash table into two layers. FKS hashing is a perfect hashing scheme where each bucket with $n$ elements is allocated $n^2$ slots. This assures that the augmented hash table has few collisions at the cost of extra memory usage. More theoretical information is available on its <a href="https://en.wikipedia.org/wiki/Dynamic_perfect_hashing#FKS_Scheme">Wikipedia page</a>.</p>
 
                 <p>As for the target use case of our hash table, we notice that nowadays people often manipuate huge datasets and that fast lookup time is very important. And, one of the most fundamental data structures for this purpose is a hash table. With this in mind we want to develop a hash table that achieves significant speedup with larget datasets. While many fine-tuned CPU-based hash tables are available and commonly used, GPU-based hash tables have only appeared recently in PhD-level work. Therefore, we would like to explore if we can utilize the massive parallel computing power of GPU in order to achieve significant improvements in read/write/delete throughput.</p>

                 <p>
                    For this project, parallelism is a means to achieve an end. In order to achieve high throughput improvements, we need parallelism for higher level of concurrency and rebalancing for more even distribution of the workload among the GPU blocks. Meanwhile, we still need to ensure that our hash table has all the properties of a CPU-based one (e.g. atomicity) by applying synchronization primitives. As learnt in the course, synchronization overhead can be significant for a highly-contended shared resource, and thus we debate and select the most efficient synchronization primitives for our hash table. 
                </p>
            </div>
            <a href="#three" class="goto-next scrolly">Next</a>
        </div>
    </section>

    <!-- Third One -->
    <section id="third" class="main special">
        <div class="container">
            <span class="image fit primary"><img src="images/test1.jpg" alt="" /></span>
            <div class="content">
                <header class="major">
                    <h2>Approach</h2>
                    </header>
                    <p>While designing our approach, we aimed to tackle every weakness of naive cuckoo hashing individually. As argued in the previous section, one of the main issues is that cuckoo hashing exhibits little natural locality. Thus, we break the original flat hash table into different buckets (with FKS hashing). By doing so, we reduce the range in which memory reads/writes jump around as key-values pairs are evicted and substituted in, thereby improving cache locality. Another issue we mentioned is that different steps in cuckoo hashing are dependent. Unfortunately, there is little we can do about that, but we can compensate it by performing hash table operations <i>by batches</i>. The remainder of the section mainly discusses how we parallelized the bulk operations on the hash table.</p>

                    <p>Overall, our hash table has a two-layer structure, with FKS hashing on top of individual cuckoo hash tables. Upon a <i>batch insert</i> operation, each input element first finds a FKS bucket and then performs cuckoo hashing with other elements in the same bucket. If no viable layout for the bucket is found, we rehash the entire bucket after providing the bucket with new hash functions (by generating new random seeds). In general, we parallelize over the entries in batch operations given by the user, and our final product is a library that can be used as long as a GPU exists. The majority of our hash table code is written by using the thrust library for CUDA. The reason for our decision is that we wanted to rapidly prototype through different algorithms and implementations in order to determine which one to select. While the rough ideas behind our algorithm have been described in the PhD thesis by Dan Alcantara, our implementation and tuning is entirely original, and thus we wanted to be able to switch rapidly in case something we attempt was headed in the wrong direction.</p>

                    <p>Our batch workloads are broken into blocks which are then processed by device kernels that modify the underlying data structures of our hash map. While cuckoo hashing is intrinsically sequential, we were able to parallelize it in two ways:
                    <ul>
                    <li>Our first implentation pushes entries into the hash map "optimistically" regardless of the state of previous attempts. This approach has more parallelism, but the cuckoo hashing is not performed in the correct order ($h_0(k), h_1(k),\cdots$), thereby causing a lot of spurious collisons and bucket rehashing. We later noticed that if we randomized the cuckoo hashing order we could do more operations in parallel, and we though that if we allowed more cuckoo hashing iterations then the hash table would converge to a layout that fit all the inputs. However, we were being too optimistic, and in fact "randomized cuckoo hashing" seemed to stall the progress with a high chance, such that very few buckets could find a satisfactory layout.<br /></li><br />

                    <li>Our second attempt was that at first we wanted the buckets to be worked upon in sequence, in order to exploit cache locality. By keeping the bucket size low, we hoped that the bucket would fit inside the cache, and the shuffling caused by cuckoo hashing would experience a speedup. However, this speedup turned out to be overpowered by the extra cost of sorting the input entries by their bucket number, because NVIDIA Visual Profiler (<code>nvvp</code>) indicated that our system was taking 80% of the time in the GPU merge-sorting the array. This meant we were losing time in general. Meanwhile, we also noticed that we needed to have a large bucket size; otherwise the hash value space would be compressed into $[0, \texttt{BUCKET\_SIZE})$, thereby increasing the rate of "false" hash value collisions sharply.</li>
                    </ul>

                    <p>After deciding on our final implementation we had many parameters to fine-tune and optimize, including how full the buckets normally are, when to make more buckets, when to coalesce old buckets, how many loop iterations we allow cuckoo hashing to run through before rehashing the bucket with new hash functions. Our final code contains the parameter set that gave the most optimal result for our test traces, but we understand that the optimal parameters may vary depending on the usage characteristics.</p>
                </header>
           </div>
            <a href="#four" class="goto-next scrolly">Next</a>
        </div>
    </section>

 
    <!-- Four One -->
    <section id="four" class="main special">
        <div class="container">
            <span class="image fit primary"><img src="images/test1.jpg" alt="" /></span>
            <div class="content">
                <header class="major">
                    <h2>Challenge</h2>
                </header>
                <p>Speed is the name of the game. Very intergral to our design is the speed of operations and the idea that what we are working with the largest datasets. The main challenge for us is to make sure that our design works for different workloads and access patterns. Also, in order to reduce the amount of CUDA code that we write manually, we need to learn how to use <a href="http://docs.nvidia.com/cuda/thrust/">Thrust library</a>.</p>
                <p>Meanwhile, in order to gain enough background knowledge on efficient implementations of hash table, we need to read a lot of papers and dissertations, which are listed in the references section. We mainly focus on papers about open addressing (especially cuckoo hashing) and lock-free hash table implemetations; meanwhile, we avoided browsing code repos on Github such that we could come up with our own design entirely without being influenced by existing solutions.</p>
                <p>As we read more about cuckoo hashing, we noticed that it is intrinsically a sequential algorithm. More specifically, each key can be potentially hashed into multiple locations, and collisions are resolved by forced eviction of the previous key. Therefore, it can be tricky to parallelize different conflicting "key insertion" requests, each of which having strong side-effects.</p>
            </div>
            <a href="#five" class="goto-next scrolly">Next</a>
        </div>
    </section>

    <!-- Fifth One -->
    <section id="five" class="main special">
        <div class="container">
            <span class="image fit primary"><img src="images/test1.jpg" alt="" /></span>
            <div class="content">
                <header class="major">
                    <h2>Results</h2>
                    </header>
                    <p> There are three main metrics that are used to categorize hash tables. Lookup speed, memory footprint, and construction time. Most important to us is lookup speed, this was our common case. Lookup speed is measured in throughput per input elements and this graph can be found at the top of the page with a link to the rest of our graphs. Cuckoo hashing has O(1) lookup time and it is very noticable when doing a lookup of a large amount of elements. </p>
                    <br />
                    <p> In order to test we created traces and ran them through construction, insertion, deletion, and lookup. If our hash function is as claimed then the input should not matter it should always hash evenly between the buckets. In the real world the user would include our library in their project and pass in as arguments the input vectors to our hash table. </p>
                    <br />
                    <p> There were two main factors in limiting our speedup. The first is the more obvious of the factors, we run our commands on large input arrays and therefore have to copy over the input array into device code in order to run out hashing algorithms on it. With very large input size this will make us memory bound. Not only then but we also move large chunks from device memory to host memory when returning the results to the user. The second slowdown major slowdown comes from divergence. In cuckoo hashing when the first bucket hash fails all the blocks move onto the second bucket hash. Doing testing we found out that ~70% of our elements found a spot after the first hashing and therefore diverged from the othe 30% that had not found a block. This happens again before we finally converge the blocks.
                   <br />
                   <p> Two other factors limiting speedup is the lack of locality when doing the thrust operations there was little locality to be found. To solve this we would of had to use sort which was more expensive then the locality issues it solved. Finally we have atomic increments and decrements to variables which slowed down the system but were essential to the correctness of the system. </p>
                </header>
          </div>
            <a href="#six" class="goto-next scrolly">Next</a>
        </div>
    </section>

    <!-- Six One -->
    <section id="six" class="main special">
        <div class="container">
            <span class="image fit primary"><img src="images/test1.jpg" alt="" /></span>
            <div class="content">
                <header class="major">
                    <h2>Resources</h2>
                </header>
                <p>The original idea of the project comes from a pair of Berkeley assignments (<a href="http://www-inst.eecs.berkeley.edu/~cs162/fa12/phase3.html">1</a>, <a href="http://www-inst.eecs.berkeley.edu/~cs162/fa12/phase4.html">2</a>) on distributed hash table over multiple nodes. Since the assignments are more related to distributed systems (e.g. loda-balancing, fault-tolerance), for this project we decided to make a hash table on a single node, but based on a GPU. This is highly relevant to what has been taught in the class, as the project require us to write CUDA code and design efficient parallelism and synchronization.</p>

                <p>At the beginning, our original idea was to use <a href="http://legion.stanford.edu/">Legion</a> to implement a distributed hash table that allows automatic load-balancing. Even though Legion makes it easier to manipulate data structures in shared memory, we had major issues integrating it in our implementation, and thus we decided to abandon the idea and write the GPU-based hash table from scratch. One of the main reasons we wanted to use Legion was only because our original idea was a distributed GPU hash table. However as we were writting the distributed code we realized that the overhead for a distributed GPU hash table makes it very niche. Since GPU has less memory then the CPU then a CPU implementation with no communication overhead would outdo most implementations we could resonaly come up with. </p>

                <p>While reading about state-of-the-art implementations of hash tables based on GPU, we came across the PhD dissertation by Dan Alcantara, which gives a comprehensive discussion about hashing (including open-addressing and cuckoo-hashing) and their GPU-based implementations. From the dissertation, we mainly learned the mechanism of cuckoo hashing, and how GPU allows massive parallelism with hash tables. 
                <br />
                <a href="http://idav.ucdavis.edu/~dfalcant/downloads/dissertation.pdf">Efficient Hash Tables on the GPU</a>.
                <br />
                <br />
                Additional Resources
                <br />
                <a href="http://research.microsoft.com/en-us/um/people/hoppe/perfecthash.pdf">Perfect Spatial Hashing</a>.
                </p>
            </div>
            <a href="#seven" class="goto-next scrolly">Next</a>
        </div>
    </section>

    <!-- sven One -->
    <section id="seven" class="main special">
        <div class="container">
            <span class="image fit primary"><img src="images/test1.jpg" alt="" /></span>
            <div class="content">
                <header class="major">
                    <h2>Goals and Deliverables</h2>
                </header>
                <h5>Plan to Achieve</h5>
                <p>Our main deliverable is a fine-tuned, GPU-based hash table with two-level cuckoo hashing. We are confident that the performance of our hash table will outperform <code>boost::unordered_map</code> significantly.

                <h5>Hope to Achieve</h5>
                <p>Our strech goal is to implement a hybrid hash table. In our literature review, we realized that GPU-based hash table is bound by GPU memory; therefore, if we need to store billions of key-value pairs, GPU memory may run out, and our project would no longer be useful. However, if we can use CPU-based hash map as a "spillover area" and only keep the "heavy-hitters" in GPU, we would have a powerful hybrid system that gives much better capacity, where the GPU-based hash table is essentially a cache for CPU-based hash table. However, this is non-trivial and requires an efficient algorithms for keeping track of the heavy hitters. Therefore, we leave it as a stretch goal, and if we have extra time after completing the project, we may discuss potential strategies for implementing this hybrid approach.</p>
                <h5>Demo</h5>
                <p>The demo for our project will be a comparison chart for a CPU-based hash table implemtation and our GPU-based hash table for different workloads (e.g. increasing R/W size). We hope to show that our hash table outperforms CPU-based hash tables and is scalable within the bound of GPU memory.</p>
            </div>
            <a href="#eight" class="goto-next scrolly">Next</a>
        </div>
    </section>

    <!-- eight -->
    <section id="eight" class="main special">
        <div class="container">
            <span class="image fit primary"><img src="images/test1.jpg" alt="" /></span>
            <div class="content">
                <header class="major">
                    <h2>Platform Choice</h2>
                </header>
                <p>We need a highly-performant GPU that has a reasonable amount of memory, and thus worker nodes in  <code>latedays</code> cluster is a natural choice. Especially, the nodes with Titan X are ideal, as they offer high GPU memory and bandwidth.
                    <br />
                    <br />
                    As for language choice, we will most likely write the hash table in C++ and CUDA since we had similar experience in <a href="http://15418.courses.cs.cmu.edu/spring2016/article/4">Assignment Two</a> in this course; we will also use Thrust library to make our life easier with CUDA. For generating test traces, we will use Python as it allows easy list manipulation.</p>
            </div>
            <a href="#nine" class="goto-next scrolly">Next</a>
        </div>
    </section>


    <!-- nine -->
    <section id="nine" class="main special">
        <div class="container">
            <span class="image fit primary"><img src="images/test1.jpg" alt="" /></span>
            <div class="content">
                <header class="major">
                    <h2>Project Checkpoint</h2>
                </header>
                <p> So far we have had two main areas of focus. One of the areas of focus has been Legion, in order to achieve the desired speeds from our hashtable and not have to worry about managing memory regions ourselves we followed the advice of cmu researcher and decided to use Legion as a tool to manage our regions and deal with the low-level GPU parallelism. We have spent a lot of time learning Legion and how to get the most out of Legion understanding their implementation and how to use their namespace. A lot of the power of Legion comes from the management and declerations of regions. </p>
                <p> Our second area of focus is building a GPU hashmap. A lot of research has been done on this topic already in determining the most efficient way of hashing information on a GPU. We have already picked an algorithm based on a paper we read and are now working on porting the algorithm to Legion. </p>
                <br />
                <p>We actually have made a sizable change since proposing the project. One of the main things we realized early on was that if our idea was to implement a high performance hash-map then our current plan of two-phase commit and providing elasticity are orthogonal to that goal. For our strech goals its looking unlikely that we will achieve our strech goals. Legion is proving to be more complicated then we originally inteded and provides more options to speed up our project then we have time to implement. </p>
                <p>For our new goals for the competition we will demo a Legion based GPU hashmap that focuses heavily in performance. We most likely will present our work as a set of graphs showing the speeds for our code compared to more traditional methods of hashing at different sizes of dataset and different number of nodes and hash functions.</p>
                <p>Our current concerns are showcasing our project. In order for our project to reach the point we want it to be at we need high performance GPUs and large datasets to test it on and to profile our code on. We our currently searching for both of these and while we have found some options nothing has been ideal.</p>
            </div>
            <a href="#ten" class="goto-next scrolly">Next</a>
        </div>
    </section>

    <!-- ten -->
    <section id="ten" class="main special">
        <div class="container">
            <span class="image fit primary"><img src="images/test1.jpg" alt="" /></span>
            <div class="content">
                <header class="major">
                    <h2>Schedule (Updated)</h2>
                </header>
                <h4>Week 1 - April 8th</h4>
                <p> Familiarity with Legion + One node system with multiple Clients + Decide Design Structure </p>
                <h4>Week 2 - April 15th</h4>
                <p> Achieve desired speed with one node (Optimizations) </p>
                <h4>Week 3 April 22nd (First Half)</h4>
                <p> Legion Code Complete (nponte) Hashing Function Research (yzong) </p>
                <h4>Week 3 April 22nd (Second Half)</h4>
                <p> Debugging Legion (nponte) Integrating Hashing (yzong) </p>
                <h4>Week 4 April 29th (First Half)</h4>
                <p> Profile code (nponte) Testing Different Hash Implementations (yzong) Midterm (yzong+nponte) </p>
                <h4>Week 4 April 29th (Second Half)</h4>
                <p> Testing on Large Datasets (nponte) Benchmarking with parameters (yzong) </p>
                <h4>Week 5 May 6th (First Half)</h4>
                <p> Optimizations (nponte+yzong) </p>
                <h4>Week 5 May 6th (Second Half)</h4>
                <p> Project Report (nponte+yzong) </p>
            </div>
        </div>
        <footer>
            <p> 15-418 Spring 2016 Final Project </p>
            <ul class="copyright" style="list-style-type: none;">
                <li>Design: <a href="http://html5up.net">HTML5 UP</a>
                </li>
                <li>Demo Images: <a href="http://unsplash.com">Unsplash</a>
                </li>
            </ul>
        </footer>
    </section>

    <!-- Scripts -->
    <script src="assets/js/jquery.min.js"></script>
    <script src="assets/js/jquery.scrollex.min.js"></script>
    <script src="assets/js/jquery.scrolly.min.js"></script>
    <script src="assets/js/skel.min.js"></script>
    <script src="assets/js/util.js"></script>
    <!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
    <script src="assets/js/main.js"></script>

</body>

</html>
